---
title: 'Lesson 10: Parallel Computing'
author: bathompso
layout: edupost
categories:
  - education
  - compphys
tags:
image: 
---

Named after Intel co-founder Gordon Moore, [Moore's law](http://en.wikipedia.org/wiki/Moore%27s_law) states that number of transistors on integrated circuits will double every 2 years. It should be noted that Moore's law is an empirical observation, not a law of physics or nature. It has been estimated that, by 2015 growth will have slowed enough that transistor count will only double every 3 years.

For programmers, Moore's law was great. If your program ran too slow on your current hardware, all you had to do was wait two years, and it should run twice as fast on the newest chips.

Around 2004, Moore's law began to break down due to overheating within the chips. Packing a large amount of transistors onto small chips left little room for heat dissipation, causing increased failures. To keep up with the Moore's law pace, chip makers expanded into multi-core architecture. By only slightly increasing the number of transistors per core, the overall chip could still keep pace.

Unfortunately for programmers, it was no longer the case that you could wait 2 years and expect a double in performance. Now, you had to write your program to take advantage of the multiple cores that were provided by the hardware. Programming to take advantage of parallel computing requires a different type of thinking than everything that has been done previously in this class.

## 10.1 Introduction to Parallel Computing

### Parallel Computing Algorithms
Not all programs can be written to utilize parallel capabilities. Programs that are best suited are those that require many **independent** iterations of a ``for`` loop. These types of programs are good because individual iterations can be assigned to each of the different processing cores, unlike normal procedural codes where one operation depends on the results of a previous one. A few examples of easily parallelized programs are listed below:

#### Vector Addition

{% image lessontext vadd.png %}

When adding vectors, you are essentially looping over the vector and adding each component together. When writing a regular program, each of these element additions would happen in order, but there is no reason for that. When writing this as a parallel program, each element is assigned to a core, and completed in sequential order.

The figure to the right illustrates how the parallel vector addition algorithm works. In this scenario, we are using a quad-core processor. The addition of elements 1-4 are assigned to each of the four cores. The first core to complete the addition is the assigned the 5th element, and so on until everything is complete.

#### Matrix Multiplication

{% image lessontext mmult.png %}

Another easily parallelizable problem is matrix multiplication. Each element of the resulting array is a multiplication of a single row and column of the input arrays. Each elementâ€™s value is independent of the others, and often this multiplication requires many iterations of a loop.

The figure to the right shows how this problem would look in a parallel scenario. Where vector addition was a 1D loop, matrix multiplication is 2D. In the example, we are computing a \\( 3\times N \\) matrix product on a quad-core processor. The multiplication proceeds left-to-right and top-to-bottom, with element 2,1 being assigned to the 4th core. As multiplications are completed, the next element of the array is assigned to that core.

#### Algorithms

When writing programs to take full advantage of parallel computing resources, there are several things to consider:

* *Make sure loop iterations are independent.* Because iterations are not necessary completed in order, you cannot expect that shared variables will be dealt with in an understandable way. There are methods that will allow for the "locking" of shared variables in parallel computation, but those will not be covered here.
* *Know what problem space you are dealing with.* In the two examples above, each had different dimensions of loops. Vector addition requires only one dimension of looping, while matrix multiplication requires 2D. This will be an important factor in designing some parallel programs.
* *Make sure the program at hand is best suited for parallelization.* It takes a decent bit longer to write and debug a parallelized code than it does a serial one. If your program includes a loop which is only run a few times (or can use other fast ways of handling in serial form), it may not be the best use of your time to write a parallel code. Often problems in physics will involve hundreds or thousands (sometimes hundreds of thousands) or iterations of a loop: these are programs that will benefit greatly from parallelization. In each of the following sections, we will time various parallel programs to see what (if any) speed improvements are made.

## [10.2 ParallelPython](parallelpython/)

One of the first transistions into the parallel computing age was the introduction of multi-core and multi-thread CPUs. Originally, CPUs could only compute a single task at a time, but with the addition of multiple cores they could handle several at once. In this section we will explore how to write Python programs that can use all available CPU cores / threads on your machine.

## [10.3 OpenCL](opencl/)

Recently, there has been another revolution in computing power: the use of graphical processing units (GPUs) in general computation. For quite a while, GPUs have been used for heavy computation: rendering and drawing the complex shapes that appear on your monitor. As computer graphics became more sophisticated (animations, transitions, gaming), GPUs became more and more powerful. To handle the enourmous amount of computation involved in rendering a computer game (drawing millions of tiny polygons), GPUs began to add more and more processing units. While each processing unit ran at a small fraction of the speed of a CPU, many advanced GPUs included hundreds (or thousands!) of these units, giving GPUs (generally) more overall processing power than a CPU.

In this section we will learn how to harness the massively-parallel architecture of your GPU for use in your Python programs.


